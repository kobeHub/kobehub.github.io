<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Inno Jia</title>
    <link>https://kobehub.github.io/posts/</link>
    <description>Recent content in Posts on Inno Jia</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <lastBuildDate>Sun, 13 Jan 2019 21:05:17 +0800</lastBuildDate>
    
	<atom:link href="https://kobehub.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>[Review] Decision Tree</title>
      <link>https://kobehub.github.io/posts/review-decision-tree/</link>
      <pubDate>Sun, 13 Jan 2019 21:05:17 +0800</pubDate>
      
      <guid>https://kobehub.github.io/posts/review-decision-tree/</guid>
      <description>Decision Tree 1. 决策树的定义以及建立 决策树是一种常见的机器学习方法。以二分类问题为例，决策树模拟人类处理问题的机制，对于某一个问题进行决策时，进行一些列的判断或者子决策。对于一棵决策树来说，具有以下属性：
 每一个非叶节点都是一个属性的划分 每一个划分指向下一个决策或者指向最终结论 决策过程从根节点开始，一直到叶节点 最终的结论对应一个目标值  基本决策树算法描述：
1. 选择一个最优属性对于剩余样本进行划分，并且将该属性作为一个节点2. 重复上述过程构造后代节点3. 遇到以下条件停止： 所有实例具有相同的值 在所有的属性集中，没有属性或者是实例具有相同的值 实例已全部划分 所以关于决策树的构建，最关键的问题在于最优属性的选取，不同的决策树算法实则是不同的属性选取方式
2. Information Gain 2.1 信息熵 随着划分的进行，我们希望决策树的分支结点所包含的样本尽可能属于同一个类别，即我们希望节点的“纯度”越来越高。信息熵（Information Entropy）可以作为纯度的一个度量标准，信息熵的计算方式：对于一个样本集合D中的第k类样本所占的比例为$p_k(k = 1,2,&amp;hellip;,|y|)$,则该样本集合的信息熵为： $$ Ent(D) = - \sum _{k=1}^{|y|}p_klog_2p_k $$ 信息熵越小，纯度越高。针对二分类问题，信息熵可以形式化为：$Entropy = -p_1log_2p_1-(1-p_1)log_2(1-p_1)$
当所有的样本属于同一个类别是，信息熵值为0.
2.2 信息增益 Information Gain可以表述为，一个通过对于一个属性的划分，原来样本集熵的下降情况。是一个属性的信息增益。所以对于一个属性而言，信息增益越大，说明使用该属性划分得到的纯度提升越大。
对于离散属性a，可能具有k个不同的取值${a^1, a^2,&amp;hellip;,a^k}$ ,所以对该属性进行划分会得到k个节点，将原样本集分为k个样本$D^1 &amp;hellip; D^k$
$$ Gain(D, a) = Ent(D) - \sum _{i=1}^k \frac{|D^i|}{|D|}Ent(D^i) $$ 有了信息增益，每次选取信息增益最大的属性进行划分。
2.3 信息增益率 信息增益准则对于可取值数目较多的属性有所偏好，如果把样本的编号作为一个属性添加进去，那么一个编号只有一个样本，“纯度”最高，所以会选取该属性进行样本的划分，但是这样的决策树显然不具有泛化能力，无法进行有效预测。
所以可以使用信息增益率作为最优属性的选择标准： $$ Gain\_ ratio(D, a) = \frac{Gain(D, a)}{IV(a)} \</description>
    </item>
    
    <item>
      <title>[Review] Bayesian decision theory</title>
      <link>https://kobehub.github.io/posts/review-bayesian-decision-theory/</link>
      <pubDate>Sun, 13 Jan 2019 17:11:19 +0800</pubDate>
      
      <guid>https://kobehub.github.io/posts/review-bayesian-decision-theory/</guid>
      <description>Bayesian decision theory 1. 常用的贝叶斯决策规则  基于最小错误率的贝叶斯决策 基于最小风险的贝叶斯决策 在限定一类错误率条件下使另一类错误率最小的两类别决策 最小最大决策  2.基本概率公式  条件概率：$p(A|B)$表示事件A在事件B发生的条件下发生的概率
 联合概率：$p(A. B)$ 表示两个事件同时发生的概率，当两个随机事件相互独立时，联合概率等于概率的乘积
  基本条件概率公式： $$ P(A | B) = \frac{P(A, B)}{P(B)} $$ 所以当A B相互独立时，$P(A|B) = P(A)$ .
 全概率公式： 如果B是由相互独立的事假组成的概率空间 ${B_1, B_2,&amp;hellip;,B_n}$, 那么关于事件A的全概率公式可以写作：  $$P(A) = \sum_{i=1}^{n}P(B_i)P(A|B_i)$$
对于全概率公式可以通过，条件概率进行推导，相当于将所有在B的子事件和同时A发生的概率的和，因为B的概率空间之和为1，所以最终得到的是A发生的概率。
 贝叶斯公式：   $$ P(B_i | A) = \frac{P(B_i)P(A|B_i)}{\sum _{i=1}^{n}P(B_i) P(A|B_i) } $$
其中 $P(B_i|A)$ 被称为后验概率，$P(B_i)$ 为先验概率，$P(A|B_i)$ 是条件概率
3. 基于最小错误率的贝叶斯决策 模式分类问题中，基于减少分类错误的要求。使用概率论中的贝叶斯公式，可得出使得分类错误率规则。
以简单的二分类任务为例，类别的状态用变量$w$表示，其中$w_1$表示正常，$w_2$表示异常。$p(w_1), p(w_2)$是已知的先验概率。那么有：</description>
    </item>
    
    <item>
      <title>[Review] Feature selection and feature extraction</title>
      <link>https://kobehub.github.io/posts/review-feature-selection-and-feature-extraction/</link>
      <pubDate>Sat, 12 Jan 2019 23:50:37 +0800</pubDate>
      
      <guid>https://kobehub.github.io/posts/review-feature-selection-and-feature-extraction/</guid>
      <description>特征选择与特征提取 1. 特征介绍 通常用于机器学习任务的特征集，可以使用一个 $l$ 维向量表示。称之为特征向量
$x = [x_1, x_2, &amp;hellip;, x_l]^T$
对于特征的产生，可以通过先验知识手工设定，也可以通过学习自动获取。对于先验知识较为明确，区分度大的样本，可以直接设计较好的特征将其分开；对于先验知识不明确，的特征集可以通学习获取。
继续以分类系统为例：针对一个典型的分类系统的基本步骤，有关特征工程（Feature Engineering）主要有两部分：
 特征产生 （feature generation）  特征设计 （design） 特征提取 （extraction）  特征选择  子集搜索 （subset search） 子集评估 （subset evaluation）   2. 特征选取 2.1 选取准则 选取与任务相关度最高的特征集，特征间相关性较低
特征与任务的相关性越大越好，特征间的相关性越小越好
2.2 特征选择的一般步骤  从原始的特征集出发对于子集进行搜索 对于特征子集进行评估 观察是否达到临界条件，达到即可以使用，否则继续上述操作  2.3 特征选取方法（ Relief / LVW / L1-Norm ） a. 过滤式（Filter）  在训练之前进行，与分类器无关。
 对数据集进行特征选取之后，根据一些评估方法，选出最优特征
 代表方法：relief。首先给定一个相关统计量（一种评估方法），可以将其理解为基于距离的，也可以是基于概率的评估。如果基于距离，如果只选取一个特征作为特征集，相当于把所有的数据放在一个一维的直线上，我们希望类内尽可能聚集，类间尽可能分离，也就是说希望 类间距离 与 类内距离 的比值尽可能大。可以计算该比值。
  现在可以设置一个阈值t 如果对于一个特征而言，经过评估其比值超过该阈值，就可以将该特征加入特征集。</description>
    </item>
    
    <item>
      <title>[Review] Main concept of ML</title>
      <link>https://kobehub.github.io/posts/review-main-concept-of-ml/</link>
      <pubDate>Sat, 12 Jan 2019 17:43:29 +0800</pubDate>
      
      <guid>https://kobehub.github.io/posts/review-main-concept-of-ml/</guid>
      <description>机器学习的一些重要概念 1. 机器学习系统的主要操作 对于一个基于普通机器学习方法的分类系统，可能具有以下的基本操作步骤：
 数据收集(Sensing)
 数据预处理(Preprocessing):
  通常使用分割操作，将分类目标与背景进行分离
 特征定义以及特征提取(Feature definition and Extraction)  以上过程一般都需要使用到一定的先验知识(Prior Knowledge)
 选取模型(Model Selection)
 训练(Training)
 模型评估(Evaluation)
  2.机器学习的主要任务 有标记数据的分类(Classification)以及回归(Regression)，无标记数据进行聚簇分析(Clustering Analysis),异常检测(Anomaly Detection).
1. 学习器  K-Nearest Neighbor(K-NN)  在特征空间找到K个最近的邻居，选取数量最大的类别作为某个样本的预测类别。无需训练，适用于小数据量，非线性问题
 Decision Tree  通过每次选取最优特征进行进一步决策，构成了一组规则集组成的决策树，通过输入样本的特征进行分类任务。
决策过程具有良好的可理解性，对于单一因素即可决定的预测结果的问题，可以弥补基于统计的机器学习的不足。
 Support Vector Machine  在特征空间选取一个超平面，使得所有样本点到超平面的总距离最小。通过定义一个间隔(Margin),最大化Margin，选取一个合适的超平面用于分类任务。通过使用合适的变换核可以进行解决非线性问题。
在解决小规模、非线性问题上具有优势，因为对于预测起到决定性作用的，是少数边界上的向量(Support Vector)
 朴素贝叶斯(Naive Bayesian)
 Neural Networks
 最小二乘 (Least Squares)
 高斯混合模型（Gaussian Mixture Model）
 Hidden Markov Model</description>
    </item>
    
    <item>
      <title>Record of build GitHub Pages use Hugo</title>
      <link>https://kobehub.github.io/posts/record-of-build-github-pages-use-hugo/</link>
      <pubDate>Tue, 08 Jan 2019 10:10:02 +0800</pubDate>
      
      <guid>https://kobehub.github.io/posts/record-of-build-github-pages-use-hugo/</guid>
      <description>前期准备 1. 本地下载安装Hugo 基于Ubuntu 16.04安装Hugo,Hugo使用Golang实现，可以直接通过源码编译安装。也可以使用已发行的版本，对于基于Debian的linux发行版，可以使用 apt,或者 snap 包管理进行最快速的安装
apt-get install hugo snap install install hugo --channel=extended # 安装Sass/SCSS版本 源码安装：
首先需要声明一个系统的GOPATH指明默认golang程序的安装路径，export GOPATH=$HOME/go ,然后clone 到本地进行编译安装
git clone https://github.com/gohugoio/hugo.git cd hugo go install 2. 建立的站点 在工作目录下，建立一个新的站点
hugo new site Hugo-site cd Hugo-site git init  该名称可以任意更换，然后添加新的主题。初始化git仓库后添加主题。可以采用hugo-coder 主题做为新的站点主题。
git submodule add https://github.com/luizdepra/hugo-coder.git themes/hugo-coder cp themes/hugo-coder/exampleSite/config.toml config.toml  然后将样例站点的配置文件cp到主文件夹下，同时将themes/hugo-coder 文件夹下的static, layouts 文件夹复制到主目录下，进行内容的更换。
3. 编辑新的文章 hugo new posts/title.md
hugo使用markdown进行文章书写，可以使用该命令建立一篇新的文章，新的文件出现在content/posts/ 文件夹下。注意需要将文件头的draft 改为false,才可以正式发布。
4.测试站点 hugo server -D 在浏览器访问[http://localhost:1313]就可以查看网站内容。</description>
    </item>
    
  </channel>
</rss>