<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Inno Jia</title>
    <link>https://kobehub.github.io/</link>
    <description>Recent content on Inno Jia</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <lastBuildDate>Wed, 12 Jun 2019 00:00:00 +0800</lastBuildDate>
    
	<atom:link href="https://kobehub.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>C&#43;&#43; Standard Template Library (STL)</title>
      <link>https://kobehub.github.io/posts/c&#43;&#43;-standard-template-library-stl/</link>
      <pubDate>Wed, 12 Jun 2019 00:00:00 +0800</pubDate>
      
      <guid>https://kobehub.github.io/posts/c&#43;&#43;-standard-template-library-stl/</guid>
      <description>STL 容器简介 C++ STL容器中常用的容器类型可以分为四类:
 顺序性容器: 提供对于数据的序列化访问  vector list deque arrays forward_list(c++11 引入)  容器适配器: 提供了对于序列化数据的另一种访问接口  queue priority_queue stack  关联容器: 实现了对于有序数据结构的快速访问(O(logn)) （红黑树）  set multiset map multimap  无序关联容器: 提供对于无序数据的快速访问（hash）(c++11 引入)  unordered_set unordered_multiset unordered_map unordered_multimap   1. vector vector 可以看作一个动态数组，自身维护数组大小。vector在内存中是连续存储，所以可以使用迭代器(iterator)或者常规指针进行访问。vector中的数据在末尾插入，有时可能需要对于底层数组的长度进行扩展(扩展为现有大小的2倍)，所以插入时间可能不一致，删除末尾元素可以在O(1)时间内完成，不发生resize操作。对于非末尾元素的操作（插入，删除），可以在线性时间O(n)内完成（*由于进行连续存储，所以插入以及删除较慢*）.
vector 上的常见操作复杂度（效率）如下：
 随机访问——常数 O(1) 在末尾插入或移除元素——均摊常数 O(1) 插入或移除元素——与到 vector 结尾的距离成线性 O(n)  vector使用的空间大小可以通过capacity()进行查询; （c++ 11）额外的空间可以通过shrink_to_fit()函数返还给系统。
1.1 成员函数  构造函数  std::vector&amp;lt;T, Allocator&amp;gt;::vector
 vector&amp;lt;T&amp;gt;(): 默认构造函数，构造一个空容器。若不提供分配器，则从默认构造的实例获得分配器。📚 vector&amp;lt;T&amp;gt;(size_type count, const T&amp;amp; value): 构造具有count个值为value的动态数组 📚 vector&amp;lt;T&amp;gt;(size_type count): 构造具有count个值为T类型默认值的动态数组 📚 vector&amp;lt;T&amp;gt;(InputIt first, InputIt last):构造拥有范围为[first, last)的内容的容器 vector&amp;lt;T&amp;gt;(const vector&amp;amp; other): 复制构造函数 📚 vector&amp;lt;T&amp;gt;(vector&amp;amp;&amp;amp; other): 移动构造函数，实现右值向左值类型的转移 vector&amp;lt;T&amp;gt;(std::initializer_list&amp;lt;T&amp;gt; init):构造具有初始内容的容器 📚   元素访问</description>
    </item>
    
    <item>
      <title>[Review] Image Preprocess</title>
      <link>https://kobehub.github.io/posts/image-preprocess/</link>
      <pubDate>Fri, 17 May 2019 22:39:33 +0800</pubDate>
      
      <guid>https://kobehub.github.io/posts/image-preprocess/</guid>
      <description>图像预处理 1. 基本的图像处理  在空间坐标删的处理  几何变换  像素处理  代数处理（对于每一个像素点的操作）   2. 代数操作 不改变图像的几何性质，对于图像的灰度级进行加减乘除等算术运算，逐像素的操作。
2.1 生成图像叠加效果（Alpha Blending）  对于两个图像的合成可以使用加法进行叠加操作： $$ g(x, y) = \alpha*f(x, y) + \beta*f(x, y) \alpha + \beta = 1 $$ 也可以用于任何两张图像的衔接。
 使用乘法，可以进行特定区域的提取，可以将除了待提取区域的位置的像素全部置为0 $$ C = \alpha*F + (1-\alpha)B $$
  2.2 获取图像的前景 可以给定一个当前图像$I$,以及对应的背景图像$B$,然后计算两个图像之间的差异： $$ Diff(x,y) = ||I(x, y) \; - \; B(x,y)||^2 $$ 如果该差异值大于某一个特定值$T$，就可以视为前景图像.在背景保持不变的环境下，对于运动目标的检测：可以使用减法操作完成，在差图像不为零的位置表明出现过运动目标。
3.逻辑操作  AND: 等价于乘法 OR: 等价于乘法 XOR: 异或操作 NOT： 非  4.</description>
    </item>
    
    <item>
      <title>[Review] Introduction to Coomputer Vision</title>
      <link>https://kobehub.github.io/posts/introduction-to-computer-vision/</link>
      <pubDate>Sun, 12 May 2019 20:13:35 +0800</pubDate>
      
      <guid>https://kobehub.github.io/posts/introduction-to-computer-vision/</guid>
      <description>计算机视觉绪论 1. 概述 1.1 视觉可以分为：  视感觉：从分子层次和观点理解人类对于光反应的基本性质  光的物理特性 刺激视觉感受器官的程度 光作用于视网膜后经视觉系统加工产生的感觉  视知觉：最终目的从狭义上说是要能对于客观场景做出观察者有意义的解释和描述  1.2 计算机视觉的目标  建立CV系统来完成各种视觉任务 加深对于人脑是觉得掌握和理解  2. 图像基础 2.1 图像分类  模拟图像  从连续的客观场景直接观察得到，用一个二维函数$f(x, y)$来表示
 数字图像  把连续的模拟图像在*坐标空间XY*以及*性质空间F*都离散化了的图像,基本单位像素
2.2 图像的表示 函数表示 把图像看作一个由二维空间向一维空间的映射函数，有几个channel则有几个映射函数
矩阵以及向量 RGB模型 三原色模型需要使用三个灰度值的映射表示： $$ f_c(x, y) = (f_r(x, y), f_g(x, y), f_b(x, y)) $$
存储  交叉存储[R G B] 为一个基本存储单元 顺序存储  简单照明模型 Phong模型，基于RGB三基色颜色系统的Phong模型：
$$ I = ka I{pa} + \sum [ kd I{pd} cosi + Ks I{ps} cos^n \theta ] $$</description>
    </item>
    
    <item>
      <title>Docker 容器配置--多容器通信(Nginx &#43; MongoDB &#43; Tornado)</title>
      <link>https://kobehub.github.io/posts/docker-%E5%AE%B9%E5%99%A8%E9%85%8D%E7%BD%AE-%E5%A4%9A%E5%AE%B9%E5%99%A8%E9%80%9A%E4%BF%A1nginx-&#43;-mongodb-&#43;-tornado/</link>
      <pubDate>Sun, 24 Mar 2019 15:29:39 +0800</pubDate>
      
      <guid>https://kobehub.github.io/posts/docker-%E5%AE%B9%E5%99%A8%E9%85%8D%E7%BD%AE-%E5%A4%9A%E5%AE%B9%E5%99%A8%E9%80%9A%E4%BF%A1nginx-&#43;-mongodb-&#43;-tornado/</guid>
      <description>Docker 容器化部署尝试 1. 背景浅谈 作为学期中的项目实训，我们打算做一个基于 web 的 LaTex 编辑工具。为了方便环境的配置以及服务的分离，选用Docker 进行环境的搭建。初始想法是基于一个 Nginx + MongoDB + Tornado + xeLaTex 的基本架构，至少使用4个容器满足基本的需求，下面进行一下简单的记录。
2. Dockerfile 自定义容器 该项目分为四个部分，对于数据库，可以直接使用 dockerhub 中的Mongodb。不需要额外的定制，直接pull 最新版即可：
docker pull mongo:latest 对于 Nginx, 也可以直接使用官方的版本，只是添加了一些需要额外挂载的目录：
# Define mountable directories. VOLUME [&amp;#34;/etc/nginx/sites-enabled&amp;#34;, &amp;#34;/etc/nginx/certs&amp;#34;, &amp;#34;/etc/nginx/conf.d&amp;#34;, &amp;#34;/var/log/nginx&amp;#34;, &amp;#34;/var/www/html&amp;#34;] 对于 Tornado 服务端，暂时先拟定一个最为简单的Demo，对于以后的版本再进行迭代：
# Base ENVFROMpython:3.7-slim# set workdir to /appWORKDIR/app# Copy the current directory contents at /appCOPY . /app# Install needed packageRUN pip install --trusted-host pypi.python.org -r requirements.txt# Make port 80 available to the world outside this contanerEXPOSE8888# Define environment variableENVNAME World# Run app.</description>
    </item>
    
    <item>
      <title>Determines the boundary value of the int overflow</title>
      <link>https://kobehub.github.io/posts/determines-the-boundary-value-of-the-int-overflow/</link>
      <pubDate>Mon, 18 Feb 2019 12:56:03 +0800</pubDate>
      
      <guid>https://kobehub.github.io/posts/determines-the-boundary-value-of-the-int-overflow/</guid>
      <description>[Leetcode Note] 关于 int32 溢出值的判定 关于整数的处理，溢出是一个难以避免的话题。对于强类型静态语言来说，一个固定大小的整形的溢出值需要格外注意。以一个 32 bits 的整形为例：对于无符号数范围（0～4294967296），有符号数的范围（-2147483648~2147483647）。对于溢出值，不可能继续采用 32 bits整形进行存储，可以考虑使用 64 bits 整形存储。但是必须考虑一个情形，如果一个大整数，对于 64 bits 仍然溢出呢？我们如果在计算一个值之后再去估计存储它所需要的位数，未免南辕北辙，毕竟我们的工作只是判断是否溢出。
有符号判定
Leetcode problem 7 problem 8 中，一个进行数字的倒序，一个实现 atoi操作，涉及到整形的操作，而且规定整形的容量为 32 bits。进行数字倒序时，循环的每次操作得到个位数字，然后加到最终结果result上。那么溢出判断在何时进行呢？一定要在进行加法之前进行。分为两种情况进行讨论：
 result &amp;gt; INT_MAX / 10, 下次操作一定会溢出 result == INT_MAX / 10, 对于 32 bits 有符号数而言，进行下次操作时。最大可以加上7，所以当 result == INT_MAX / 10 &amp;amp;&amp;amp; digit &amp;gt; 7(digit 为下一次加上的数字) 也会溢出  对于负数预制类似。
go 实现的 pro 7：
func reverse(x int) int { var digit int32 var d = int32(x) const MAX int32 = int32(^uint32(0) &amp;gt;&amp;gt; 1) const MIN = ^MAX var sum int32 = 0 for d !</description>
    </item>
    
    <item>
      <title>Go detail to ASCII, Unicode and Character sets</title>
      <link>https://kobehub.github.io/posts/go-detail-to-ascii-unicode-and-character-sets/</link>
      <pubDate>Wed, 13 Feb 2019 13:53:00 +0800</pubDate>
      
      <guid>https://kobehub.github.io/posts/go-detail-to-ascii-unicode-and-character-sets/</guid>
      <description>ASCII，Unicode 码与字符集 很早就对于编码方式，字符集等问题心存疑惑。在一些语言中，有关字符集的问题都被进行了简化。但是在一些语言中，比如 Rust，对于这个问题进行了较为接近本质的谈论。
1. 历史溯源 很久很久以前，有一群人，他们决定使用8个可以开合的晶体管来组合形成不同的状态，以表示世界上的万物。他们看见8个字节的开关状态最好，于是将其称为“字节”。后来产生了可以处理这些字节的工具，也就是计算机。最早计算机只在美国使用，八位的字节一共可以组合出256种不同的状态，他们使用了0-127进行编码。这样计算机就可以处理英文的文字了。这样的编码方案被称为 ASCII（America Standard Code for Information Translation）码。由于自己聚具有多大8bits的空间，所以很多人都在思考：“gosh， 我们可以使用128-255的编码”当很多人同时具有这个想法就很糟糕了，很多公司使用各自的规范，这造成了极大地混乱。除此之外，ASCII还有更大的先天不足，由于仅支持英文，所以对于世界上的其他语言无法进行表示。
但是简单地认为“plain text = ascii = characters 为8bits”的错误大错特错。为了对于世界上其他语系进行有效支持，必须使用更多的字节，并且使用统一的编码。由此，Unicode这一统一的编码集便应运而生了。在这里必须明确一个概念，ASCII， Unicode都是字符集，utf-8是一种编码方式，通过使用统一的编码方式，依据unicode字符集，可以满足所有的编码需求。
2. ASCII 码及其发展 使用一个字节表示进行编码，把其中0-31种状态用作特殊用途，0x20一下字段被称为控制码，然后将空格、标点、数字、大小写字母分别用连续的字节状态表示，一直编码到127号。
 0x20&amp;ndash;&amp;gt; space 0x41 &amp;ndash;&amp;gt; A  我们们可以这样理解ASCII码，她就是一个字典，是英文世界和计算机世界沟通的通道。这个通道让一开始的沟通变顺畅。就这样计算机和程序员愉快的交流着。
英文使用128个编码就足够了，但是用于其同语言的编码，必须对于128之后的码点进行编码，或者使用新的字符集。
后来，就像建造巴比伦塔一样，世界各地都开始使用计算机，但是很多国家用的不是英文，他们的字母里有许多是ASCII里没有的，为了可以在计算机保存他们的文字，他们决定采用 127号之后的空位来表示这些新的字母、符号，还加入了很多画表格时需要用下到的横线、竖线、交叉等形状，一直把序号编到了最后一个状态255。从128 到255这一页的字符集被称”扩展字符集“。
等中国人们得到计算机时，已经没有可以利用的字节状态来表示汉字，况且有6000多个常用汉字需要保存呢。但是这难不倒智慧的中国人民，我们不客气地把那些127号之后的奇异符号们直接取消掉, 规定：一个小于127的字符的意义与原来相同，但两个大于127的字符连在一起时，就表示一个汉字，前面的一个字节（他称之为高字节）从0xA1用到0xF7，后面一个字节（低字节）从0xA1到0xFE，这样我们就可以组合出大约7000多个简体汉字了。在这些编码里，我们还把数学符号、罗马希腊的字母、日文的假名们都编进去了，连在 ASCII 里本来就有的数字、标点、字母都统统重新编了两个字节长的编码，这就是常说的”全角”字符，而原来在127号以下的那些就叫”半角”字符了。中国人民看到这样很不错，于是就把这种汉字方案叫做 “GB2312“。GB2312 是对 ASCII 的中文扩展。但是中国的汉字太多了，为了满足需求，干脆不再要求低字节一定是127号之后的内码，只要第一个字节是大于127就固定表示这是一个汉字的开始，不管后面跟的是不是扩展字符集里的内容。结果扩展之后的编码方案被称为 GBK 标准，GBK包括了GB2312 的所有内容，同时又增加了近20000个新的汉字（包括繁体字）和符号。 后来少数民族也要用电脑了，于是我们再扩展，又加了几千个新的少数民族的字，GBK扩成了 GB18030。
由于使用双字节进行编码，形成了 DBCS(Double Byte Character Set)双字节字符集，所以才有了“一个中文算两个英文字符&amp;hellip;&amp;hellip;&amp;hellip;”
3. Unicode 码 由于每个国家都试图编写自己的编码标准，导致了互相之间都不支持。此时，ISO着手解决了这个问题：废了所有的地区性编码方案，重新搞一个包括了地球上所有文化、所有字母和符号 的编码！他们打算叫它”Universal Multiple-Octet Coded Character Set”，简称 UCS, 俗称 “unicode“。
Unicode 是一次伟大的尝试，希望可以创造包含世界上所有有意义符号统一字符集。有人误以为，unicode 码使用两个字节，每一个编码使用16 bits 进行编码，所以最多可以编码 65,536 个字符。这实际上是不对的。事实上，Unicode有一种不同的思考角色的方式，你必须要理解Unicode的思维方式否则没有任何意义。</description>
    </item>
    
    <item>
      <title>Yet another tool to mock interfaces in Go</title>
      <link>https://kobehub.github.io/posts/yet-another-tool-to-mock-interface-in-go/</link>
      <pubDate>Fri, 25 Jan 2019 14:16:20 +0800</pubDate>
      
      <guid>https://kobehub.github.io/posts/yet-another-tool-to-mock-interface-in-go/</guid>
      <description>Go 语言中一个模拟接口的工具 单元测试作为一种强大的工具，可以检查代码各个方面的行为。如果对进行代码测试十分重视，那么将会一直编写可持续、可维护的代码，并且在代码的实现过程中保持代码的完整性。依赖于抽象的、经过开发者精心设计的代码是很容易进行测试的，所以代码的可测试性也作为其质量的一个指标。
如果你已经在 Go 中尝试过测试代码，可能知道接口的巨大作用。在 Go 的标准库中，提供了一系列接口，这些接口大多数只包含一个方法，可以直接使用这些接口。
Go 还有一个补充框架，用以模拟接口。同时，还有一些社区驱动的包可以完成类似的功能。他们中的大多数都可以根据给定接口，生成实现这些接口的 struct 。 对于较大的接口，或者嵌套了其他接口，使用这种方式很有效。当接口只有一个方法时，不是更有效果吗？
关于 Go 中的接口，最令人惊讶的部分是它的默认满足性。任何类型，只需要提供其签名与接口声明中的方法匹配的实现，即可以满足该接口。这种类型甚至可以是函数，如果熟悉 net/http 包，你也可能看到其中的一种可以叫做 adapters 的类型。
// A Handler responds to an HTTP request. type Handler interface { ServeHTTP(ResponseWriter, *Request) } // The HandlerFunc type is an adapter to allow the use of // ordinary functions as HTTP handlers. If f is a function // with the appropriate signature, HandlerFunc(f) is a // Handler that calls f.</description>
    </item>
    
    <item>
      <title>Introduce to Convolutional neural network</title>
      <link>https://kobehub.github.io/posts/introduce-to-cnn/</link>
      <pubDate>Tue, 22 Jan 2019 23:13:21 +0800</pubDate>
      
      <guid>https://kobehub.github.io/posts/introduce-to-cnn/</guid>
      <description>CNN简介 1. 特征 Background:
深度学习的基本原理是基于人工神经网络，信号从一个神经元进入，经过非线性的activation function,
传到下一层神经元；再经过该层神经元的激活，继续向下传递，如此循环往复到输出层，正是由于这些非线性函数的反复叠加，才使得神经网络有了足够的capacity来抓取复杂的pattern
所以使用activation function是必须的，如果不使用激活函数，每层的输出都是上一层的线性组合，与没有隐藏层效果相当，这种情形就是最原始的感知机(Preceptron)
Regular Neural Networks(RNN):
一个神经网络的最简单的结构包括输入层，隐含层，输出层，每一层上有多个神经元，上一层的神经元通过激活函数映射到下一层的神经元，每个神经元之间有相应的权值，普通的神经网络架构都是全连接的，即每一层的每个神经元的输入包含上一层的全部输出
Convolutional Neural Networks(CNNs/ConvNets):
卷积神经网络与普通神经网络相似，由神经元构成，具有可学习的weights and biases，每个神经元接收一个输入进行点积运算后，并且可以选择非线性跟随．整个网络仍然表现出单一的可微分分数函数：
从一端的原始图像像素(raw pxiel)到另一端的分数函数
并且在最后一层（全连接层）仍然具有损失函数(SVM/Softmax) ConvNet体系结构明确地假设输入是图像，这允许我们将某些属性编码到体系结构中。这些使得forward 函数　更有效地实施并极大地减少了网络中的参数数量
2.与全连接神经网络的比较 对于一个32*32*3的图像而言，如果采用RNN(regular neural nets) 将其转化为一维矩阵，如果采用全连接架构，对于每一个像素都需要一个神经元　所以普通神经网络的一层将需要　32*32*3= 3027 个权重
不适合进行图像的处理，而且过多的参数极易导致过拟合
使用神经网络:
 特征提取的高效性 数据格式简易性 参数数目少量性  卷积神经网络每一层使得其神经元分布在三个维度上，每一层把一个3-D的volume输入转化为另一个3-D volume．图中的width, height代表了图像的维度大小，depth为3代表色素通道(R G B 3 channels)
 A ConvNet is made up of Layers. Every Layer has a simple API: It transforms an input 3D volume to an output 3D volume with some differentiable function that may or may not have parameters.</description>
    </item>
    
    <item>
      <title>[Review] Neural Networks</title>
      <link>https://kobehub.github.io/posts/review-neural-networks/</link>
      <pubDate>Mon, 14 Jan 2019 20:23:39 +0800</pubDate>
      
      <guid>https://kobehub.github.io/posts/review-neural-networks/</guid>
      <description>神经网络与深度学习 1. Artificial Neural Networks 1.1 M-P Neuron Model 由生物神经元的启发，McCulloch and Pitts在1943年提出了MP神经元模型。神经元模型是一个包含输入，输出与计算功能的模型。
输入可以类比为神经元的树突，而输出可以类比为神经元的轴突，计算则可以类比为细胞核。神经元接收来自n个其他神经元传递过来的输入信号，这些输入信号通过带权重的连接进行传递，神经元接收到的总输入值将与神经元的阈值进行比较，然后通过激活函数的处理产生神经元的输出。这是仅有一个神经元的模型。
激活函数：
阶跃函数，输出值大于0则为1，否则为0 $$ sgn(x)= \begin{cases} 1, &amp;amp; x \geq 0; \\ 0, &amp;amp; x &amp;lt; 0; \end{cases} \\y= \begin{cases} 1, &amp;amp; \sum_{i=1}^{n}w_ix_i-b \geq 0; \\0, &amp;amp; otherwise; \end{cases} $$
1.2 Perceptron 第一个可以根据输入样本学习权重的模型，是首个可以学习的人工神经网络。The Perceptron was introduced in 1958 by Frank Rosenblatt.
该模型有两个层次。分别是输入层和输出层。输入层里的“输入单元”只负责传输数据，不做计算。输出层里的“输出单元”则需要对前面一层的输入进行计算我们把需要计算的层次称之为“计算层”，并把拥有一个计算层的网络称之为“单层神经网络”。单层感知机只可以处理线性可分的数据。
多层感知机（MLP）
多层感知机具有一个或者多个隐含层，不同的层之间神经元是全连接的。这是一个典型的前馈网络（Feedforward ）,在该网络中，信息只朝着一个方向移动。
深度学习之父杰弗里·辛顿（Geoffrey Hinton），其在1986年发明了适用于多层感知器（MLP）的BP算法（反向传播算法），并采用Sigmoid激活函数进行非线性映射，有效解决了非线性分类和学习的问题。
Activate Function:
Sigmoid 函数，与用于分类问题的线性回归中使用的联系函数相同。
1.3 LeNet 1989 年。 Yann LeCun（乐困）利用反向传播的思想发明了卷积神经网络-LeNet，并将其用于数字识别，且取得了较好的成绩。BP算法被指出存在梯度消失问题，即在误差梯度反向传递的过程中，误差梯度传到前层时几乎为0，因此无法对前层进行有效的学习。</description>
    </item>
    
    <item>
      <title>[Review] Regression</title>
      <link>https://kobehub.github.io/posts/review-regression/</link>
      <pubDate>Mon, 14 Jan 2019 15:35:50 +0800</pubDate>
      
      <guid>https://kobehub.github.io/posts/review-regression/</guid>
      <description>线性回归 1. 回归模型 回归任务主要是处理连续值的模型。回归学习是用来估计输入值和输出值之间的关系，建立输入值和输出值之间的数学模型，当再来一个新样本时，可以预测新样本的输出值。回归是一个有监督的学习过程，在建立输入值和输出值之间的数学模型时，需要有训练集，在预测的时候需要一些先验知识。
典型例子：房价的估计
分类 1. 按照输入值的数量
 单元回归，输入的特征值只有一个 多元回归，输入多个特征值  2. 输入与输出的关系
 线性回归 非线性回归  基本模型： $$ f(x_i) = wx_i + b $$ 最常用的性能度量是均方误差，可以试图通过令均方误差最小化求得参数。
2.最小二乘法 2.1 单元线性回归 对于只有一维特征的线性回归，可以直接使用均方误差作为损失函数。同时，对于$w, b$求偏导。令其等于0，即可以求出模型的两个参数。
2.2 多元线性回归 更一般的情形是对于一个数据集D由d个属性进行描述，所以输入的特征是d维特征，此时试图学得的模型可以表示为： $$ f(x_i) = w^T x_i + b $$ 可以把参数$w^T ,b吸收入向量的形式 \hat{w} = (w^T; b)$,同时把数据集D表示为一个m * (d+1) 的矩阵形式，其中每一行代表一个样本，每一列代表一个属性，同时把标记也写作向量形式 m * 1
优化目标： $$ \hat{w} = argmin _{ \hat{w} } (y-X \hat {w})^T (y - X \hat {w}) \\ E _{ \hat {w}} = (y-X \hat {w}) ^T(y - X \hat {w}) $$ 求偏导： $$ \frac{ \partial E}{\partial \hat{w}} = 2 X^T(X \hat{w} - y) $$ 令上式等于0可以得到参数的最优解，但是由于涉及矩阵的逆，所以需要进行讨论。如果，$X^TX$是一个满秩矩阵或者正定矩阵，可以直接求： $$ \hat{w}^* = (X^TX)^{-1}X^Ty $$ 但现实任务往往不是满秩矩阵（没有一行或者一列为０），就会有多个解，这是需要使用正则化的方法。使用正则化技术，减少特征前面的参数w的大小，具体就是修改线性回归中的损失函数形式即可，岭回归以及Lasso回归就是这么做的。或者丢弃一些对我们最终预测结果影响不大的特征，具体哪些特征需要丢弃可以通过PCA算法来实现；</description>
    </item>
    
    <item>
      <title>[Review] Support Vector Machine</title>
      <link>https://kobehub.github.io/posts/review-support-vector-machine/</link>
      <pubDate>Mon, 14 Jan 2019 12:01:51 +0800</pubDate>
      
      <guid>https://kobehub.github.io/posts/review-support-vector-machine/</guid>
      <description>SVM 1. Margin and Support Vector 首先考虑一个线性分类任务,分类的最基本的想法就是在一个样本空间中划分一个超平面,将两个不同的类别的样本区分开。如何寻找超平面以及衡量超平面的好坏成为重要的问题。
为了正确的进行分类，我们需要找到一个鲁棒性更强的超平面，对于新的数据具有更强的泛化能力。在样本中，距离超平面的最近的样本点的距离应该尽可能的大，依据此规则选取超平面。
间隔:margin, 两类样本到划分超平面的最近的距离之和。代表着划分超平面对于样本局部扰动的”容忍性“，间隔越大，容忍性越好。
超平面可以用以下形式表示： $$ W^Tx+b = 0 $$ 根据超平面可以对样本进行划分： $$ \begin{cases} w^T x_i + b \ge 1, \qquad &amp;amp; y_i = +1 \\ w^Tx_i + b \le 1, \qquad &amp;amp; y_i = -1 \end{cases} $$ 支持向量： 距离超平面最小的训练样本使得等号成立，被称为支持向量
Margin: $\gamma = \frac{2}{||w||}$,
Target:最大化margin
为了找到具有最大间隔的超平面，使用以下目标函数： $$ \begin{align} &amp;amp; min_{w,b} \frac{1}{2} ||w||^2 \qquad (1) \\ &amp;amp; s.t. y_i(w^Tx_i+b) \ge 1, \qquad i=1,2,&amp;hellip;,m \end{align} $$ 这是支持向量机的基本模型。</description>
    </item>
    
    <item>
      <title>[Review] Decision Tree</title>
      <link>https://kobehub.github.io/posts/review-decision-tree/</link>
      <pubDate>Sun, 13 Jan 2019 21:05:17 +0800</pubDate>
      
      <guid>https://kobehub.github.io/posts/review-decision-tree/</guid>
      <description>Decision Tree 1. 决策树的定义以及建立 决策树是一种常见的机器学习方法。以二分类问题为例，决策树模拟人类处理问题的机制，对于某一个问题进行决策时，进行一些列的判断或者子决策。对于一棵决策树来说，具有以下属性：
 每一个非叶结点都是一个属性的划分
 每一个划分指向下一个决策或者指向最终结论
 决策过程从根结点开始，一直到叶结点
 最终的结论对应一个目标值
  基本决策树算法描述：
1. 选择一个最优属性对于剩余样本进行划分，并且将该属性作为一个结点 2. 重复上述过程构造后代结点 3. 遇到以下条件停止： 所有实例具有相同的值 在所有的属性集中，没有属性或者是实例具有相同的值 实例已全部划分 所以关于决策树的构建，最关键的问题在于最优属性的选取，不同的决策树算法实则是不同的属性选取方式
2. Information Gain 2.1 信息熵 随着划分的进行，我们希望决策树的分支结点所包含的样本尽可能属于同一个类别，即我们希望结点的“纯度”越来越高。信息熵（Information Entropy）可以作为纯度的一个度量标准，信息熵的计算方式：对于一个样本集合D中的第k类样本所占的比例为$p_k(k = 1,2,&amp;hellip;,|y|)$,则该样本集合的信息熵为： $$ Ent(D) = - \sum _{k=1}^{|y|}p_klog_2p_k $$ 信息熵越小，纯度越高。针对二分类问题，信息熵可以形式化为：$Entropy = -p_1log_2p_1-(1-p_1)log_2(1-p_1)$
当所有的样本属于同一个类别是，信息熵值为0.
2.2 信息增益 Information Gain可以表述为，一个通过对于一个属性的划分，原来样本集熵的下降情况。是一个属性的信息增益。所以对于一个属性而言，信息增益越大，说明使用该属性划分得到的纯度提升越大。
对于离散属性a，可能具有k个不同的取值${a^1, a^2,&amp;hellip;,a^k}$ ,所以对该属性进行划分会得到k个结点，将原样本集分为k个样本$D^1 &amp;hellip; D^k$
$$ Gain(D, a) = Ent(D) - \sum _{i=1}^k \frac{|D^i|}{|D|}Ent(D^i) $$ 有了信息增益，每次选取信息增益最大的属性进行划分。
2.3 信息增益率 信息增益准则对于可取值数目较多的属性有所偏好，如果把样本的编号作为一个属性添加进去，那么一个编号只有一个样本，“纯度”最高，所以会选取该属性进行样本的划分，但是这样的决策树显然不具有泛化能力，无法进行有效预测。</description>
    </item>
    
    <item>
      <title>[Review] Bayesian decision theory</title>
      <link>https://kobehub.github.io/posts/review-bayesian-decision-theory/</link>
      <pubDate>Sun, 13 Jan 2019 17:11:19 +0800</pubDate>
      
      <guid>https://kobehub.github.io/posts/review-bayesian-decision-theory/</guid>
      <description>Bayesian decision theory 1. 常用的贝叶斯决策规则  基于最小错误率的贝叶斯决策 基于最小风险的贝叶斯决策 在限定一类错误率条件下使另一类错误率最小的两类别决策 最小最大决策  2.基本概率公式  条件概率：$p(A|B)$表示事件A在事件B发生的条件下发生的概率
 联合概率：$p(A. B)$ 表示两个事件同时发生的概率，当两个随机事件相互独立时，联合概率等于概率的乘积
  基本条件概率公式： $$ P(A | B) = \frac{P(A, B)}{P(B)} $$ 所以当A B相互独立时，$P(A|B) = P(A)$ .
 全概率公式： 如果B是由相互独立的事假组成的概率空间 ${B_1, B_2,&amp;hellip;,B_n}$, 那么关于事件A的全概率公式可以写作：  $$P(A) = \sum_{i=1}^{n}P(B_i)P(A|B_i)$$
对于全概率公式可以通过，条件概率进行推导，相当于将所有在B的子事件和同时A发生的概率的和，因为B的概率空间之和为1，所以最终得到的是A发生的概率。
 贝叶斯公式：   $$ P(B_i | A) = \frac{P(B_i)P(A|B_i)}{\sum _{i=1}^{n}P(B_i) P(A|B_i) } $$
其中 $P(B_i|A)$ 被称为后验概率，$P(B_i)$ 为先验概率，$P(A|B_i)$ 是条件概率
3. 基于最小错误率的贝叶斯决策 模式分类问题中，基于减少分类错误的要求。使用概率论中的贝叶斯公式，可得出使得分类错误率规则。
以简单的二分类任务为例，类别的状态用变量$w$表示，其中$w_1$表示正常，$w_2$表示异常。$p(w_1), p(w_2)$是已知的先验概率。那么有：</description>
    </item>
    
    <item>
      <title>[Review] Feature selection and feature extraction</title>
      <link>https://kobehub.github.io/posts/review-feature-selection-and-feature-extraction/</link>
      <pubDate>Sat, 12 Jan 2019 23:50:37 +0800</pubDate>
      
      <guid>https://kobehub.github.io/posts/review-feature-selection-and-feature-extraction/</guid>
      <description>特征选择与特征提取 1. 特征介绍 通常用于机器学习任务的特征集，可以使用一个 $l$ 维向量表示。称之为特征向量
$x = [x_1, x_2, &amp;hellip;, x_l]^T$
对于特征的产生，可以通过先验知识手工设定，也可以通过学习自动获取。对于先验知识较为明确，区分度大的样本，可以直接设计较好的特征将其分开；对于先验知识不明确，的特征集可以通学习获取。
继续以分类系统为例：针对一个典型的分类系统的基本步骤，有关特征工程（Feature Engineering）主要有两部分：
 特征产生 （feature generation）  特征设计 （design） 特征提取 （extraction）  特征选择  子集搜索 （subset search） 子集评估 （subset evaluation）   2. 特征选取 2.1 选取准则 选取与任务相关度最高的特征集，特征间相关性较低
特征与任务的相关性越大越好，特征间的相关性越小越好
2.2 特征选择的一般步骤  从原始的特征集出发对于子集进行搜索 对于特征子集进行评估 观察是否达到临界条件，达到即可以使用，否则继续上述操作  2.3 特征选取方法（ Relief / LVW / L1-Norm ） a. 过滤式（Filter）  在训练之前进行，与分类器无关。
 对数据集进行特征选取之后，根据一些评估方法，选出最优特征
 代表方法：relief。首先给定一个相关统计量（一种评估方法），可以将其理解为基于距离的，也可以是基于概率的评估。如果基于距离，如果只选取一个特征作为特征集，相当于把所有的数据放在一个一维的直线上，我们希望类内尽可能聚集，类间尽可能分离，也就是说希望 类间距离 与 类内距离 的比值尽可能大。可以计算该比值。
  现在可以设置一个阈值t 如果对于一个特征而言，经过评估其比值超过该阈值，就可以将该特征加入特征集。</description>
    </item>
    
    <item>
      <title>[Review] Main concept of ML</title>
      <link>https://kobehub.github.io/posts/review-main-concept-of-ml/</link>
      <pubDate>Sat, 12 Jan 2019 17:43:29 +0800</pubDate>
      
      <guid>https://kobehub.github.io/posts/review-main-concept-of-ml/</guid>
      <description>机器学习的一些重要概念 1. 机器学习系统的主要操作 对于一个基于普通机器学习方法的分类系统，可能具有以下的基本操作步骤：
 数据收集(Sensing)
 数据预处理(Preprocessing):
  通常使用分割操作，将分类目标与背景进行分离
 特征定义以及特征提取(Feature definition and Extraction)  以上过程一般都需要使用到一定的先验知识(Prior Knowledge)
 选取模型(Model Selection)
 训练(Training)
 模型评估(Evaluation)
  2.机器学习的主要任务 有标记数据的分类(Classification)以及回归(Regression)，无标记数据进行聚簇分析(Clustering Analysis),异常检测(Anomaly Detection).
1. 学习器  K-Nearest Neighbor(K-NN)  在特征空间找到K个最近的邻居，选取数量最大的类别作为某个样本的预测类别。无需训练，适用于小数据量，非线性问题
 Decision Tree  通过每次选取最优特征进行进一步决策，构成了一组规则集组成的决策树，通过输入样本的特征进行分类任务。
决策过程具有良好的可理解性，对于单一因素即可决定的预测结果的问题，可以弥补基于统计的机器学习的不足。
 Support Vector Machine  在特征空间选取一个超平面，使得所有样本点到超平面的总距离最小。通过定义一个间隔(Margin),最大化Margin，选取一个合适的超平面用于分类任务。通过使用合适的变换核可以进行解决非线性问题。
在解决小规模、非线性问题上具有优势，因为对于预测起到决定性作用的，是少数边界上的向量(Support Vector)
 朴素贝叶斯(Naive Bayesian)
 Neural Networks
 最小二乘 (Least Squares)
 高斯混合模型（Gaussian Mixture Model）
 Hidden Markov Model</description>
    </item>
    
    <item>
      <title>Project</title>
      <link>https://kobehub.github.io/projects/</link>
      <pubDate>Tue, 08 Jan 2019 20:24:57 +0800</pubDate>
      
      <guid>https://kobehub.github.io/projects/</guid>
      <description>Waiting to be updated&amp;hellip;</description>
    </item>
    
    <item>
      <title>Contact</title>
      <link>https://kobehub.github.io/contact/</link>
      <pubDate>Tue, 08 Jan 2019 17:28:13 +0800</pubDate>
      
      <guid>https://kobehub.github.io/contact/</guid>
      <description> Email:jdgets111@gmail.com
 WeChat: Add Contacts jdgets, or scan the QR code in WeChat
  </description>
    </item>
    
    <item>
      <title>Record of build GitHub Pages use Hugo</title>
      <link>https://kobehub.github.io/posts/record-of-build-github-pages-use-hugo/</link>
      <pubDate>Tue, 08 Jan 2019 10:10:02 +0800</pubDate>
      
      <guid>https://kobehub.github.io/posts/record-of-build-github-pages-use-hugo/</guid>
      <description>前期准备 1. 本地下载安装Hugo 基于Ubuntu 16.04安装Hugo,Hugo使用Golang实现，可以直接通过源码编译安装。也可以使用已发行的版本，对于基于Debian的linux发行版，可以使用 apt,或者 snap 包管理进行最快速的安装
apt-get install hugo snap install install hugo --channel=extended # 安装Sass/SCSS版本 源码安装：
首先需要声明一个系统的GOPATH指明默认golang程序的安装路径，export GOPATH=$HOME/go ,然后clone 到本地进行编译安装
git clone https://github.com/gohugoio/hugo.git cd hugo go install 2. 建立的站点 在工作目录下，建立一个新的站点
hugo new site Hugo-site cd Hugo-site git init  该名称可以任意更换，然后添加新的主题。初始化git仓库后添加主题。可以采用hugo-coder 主题做为新的站点主题。
git submodule add https://github.com/luizdepra/hugo-coder.git themes/hugo-coder cp themes/hugo-coder/exampleSite/config.toml config.toml  然后将样例站点的配置文件cp到主文件夹下，同时将themes/hugo-coder 文件夹下的static, layouts 文件夹复制到主目录下，进行内容的更换。
3. 编辑新的文章 hugo new posts/title.md
hugo使用markdown进行文章书写，可以使用该命令建立一篇新的文章，新的文件出现在content/posts/ 文件夹下。注意需要将文件头的draft 改为false,才可以正式发布。
4.测试站点 hugo server -D 在浏览器访问[http://localhost:1313]就可以查看网站内容。</description>
    </item>
    
    <item>
      <title>About</title>
      <link>https://kobehub.github.io/about/</link>
      <pubDate>Mon, 07 Jan 2019 16:10:59 +0800</pubDate>
      
      <guid>https://kobehub.github.io/about/</guid>
      <description>I am Inno Jia. A junior in School of Software, Shandong University. I will graduate in 2020.
My major is Artificial Intelligence, and my interests focus on machine learning and algorithm. Now my work is mainly at signal processing. I am a programming enthusiast and willing to share and discuss.
I am a typical user of Python, and my favorite static language is Golang. In many ways, I like to learn various programming languages and skills.</description>
    </item>
    
  </channel>
</rss>