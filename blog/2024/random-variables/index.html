<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Common used random variables | Inno (Dong)  Jia</title>
    <meta name="author" content="Inno (Dong)  Jia">
    <meta name="description" content="probability, statistics, random variables, discrete and continuous.">
    <meta name="keywords" content="Digital Twin, UAV, 6G">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://kobehub.github.io/blog/2024/random-variables/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    



  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Inno (Dong)¬†</span>Jia</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">cv</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      <!-- _layouts/post.html -->

<div class="post">

  <header class="post-header">
    <h1 class="post-title">Common used random variables</h1>
    <p class="post-meta">June 17, 2024</p>
    <p class="post-tags">
      <a href="/blog/2024"> <i class="fas fa-calendar fa-sm"></i> 2024 </a>
      ¬† ¬∑ ¬†
        <a href="/blog/tag/math">
          <i class="fas fa-hashtag fa-sm"></i> Math</a> ¬†
          
      ¬† ¬∑ ¬†
        <a href="/blog/category/math">
          <i class="fas fa-tag fa-sm"></i> math,</a> ¬†
          <a href="/blog/category/statistics">
          <i class="fas fa-tag fa-sm"></i> statistics</a> ¬†
          

    </p>
  </header>

  <article class="post-content">
    <p>In this post, I am going to summarize some common used random variables for future references.</p>

<h2 id="bernoulli-distribution">Bernoulli Distribution</h2>

<p>A Bernoulli distribution is the simplest discrete probability distribution, representing a single trial that has only two possible outcomes: success (usually denoted by 1) or failure (usually denoted by 0). $ T_t $ has one parameter $ p $, which represents the probability of success.</p>

\[\begin{aligned}
    P(X=1)=p \\ 
    P(X=0)=1‚àíp
\end{aligned}\]

<p><strong>Mean and Variance</strong></p>

\[\begin{aligned}
    E(X) = p \\
    Var(X) = p (1-p)
\end{aligned}\]

<h2 id="binomial-distribution">Binomial Distribution</h2>

<p>A Binomial distribution represents the number of successes in a fixed number of independent Bernoulli trials, each with the same probability of success. The binomial distribution has parameters $(n, p)$ with $n$ as the number of trails and $p$ is the probability of success of one Bernoulli trail.</p>

<p>The PMF is:</p>

\[\begin{aligned}
X \sim \rm{Binomial}(n, p) \\
p(X = k) = \binom{n}{k}p^k(1-p)^{n-k}
\end{aligned}\]

<p><strong>Mean and Variance</strong></p>

\[\begin{aligned}
        E(X) = np \\
        Var(X) = np (1-p)
    \end{aligned}\]

<h2 id="poisson-distribution">Poisson Distribution</h2>

<p>Poisson is a discrete probability distribution to express the probability of a given number of events occurring in a fixed interval of time (i.e. the number of tasks arriving in queuing systems). These events occur with a known mean rate $(\lambda)$ and independently of the time since the last event. The Poisson distribution deals with the number of occurrences in a fixed period of time, and the exponential distribution deals with the time between occurrences of successive events as time flows by continuously.</p>

<p>The probability mass function is:</p>

\[P(X = k) = \frac{\lambda^k e^{-\lambda}}{k!}\]

<p>The positive real number $\lambda$ is equal to the expected value, and the variance of $X$:</p>

\[\lambda = \rm{E}(X) = Var(X)\]

<p>The Poisson distribution can be seen as a limit of the binomial distribution under certain conditions. This is known as the <strong>Poisson approximation to the binomial distribution</strong>.</p>

<p>The Poisson distribution is a good approximation to the binomial distribution when:</p>

<ol>
  <li>
<strong>$n$ is large</strong> (the number of trials is large).</li>
  <li>
<strong>$p$ is small</strong> (the probability of success in each trial is small).</li>
  <li>The product  $np=\lambda$ is moderate (the expected number of successes remains constant).</li>
</ol>

<p><strong>Additivity</strong>: if $X_i \sim \rm{Poisson(\lambda_i)}$, for $i = 1, 2, \dots, n$, and the $X_i$ is independent, then</p>

\[X_1 + X_2 + \dots + X_n \sim \rm{Poisson(\lambda_1 + \lambda_2 + \dots + \lambda_n)}\]

<h3 id="poisson-distribution-as-an-approximation-of-binomial-distribution">Poisson distribution as an approximation of Binomial distribution</h3>

<p>As a rule of thumb, if $n \ge 100 $ and $np &lt; 10$, the Poisson distribution (taking $\lambda = np$‚Äã‚Äã) can provide a very good approximation to the binomial distribution.</p>

\[\lim_{n\rightarrow \infty} \rm{Binomial}(x) = \frac{e^{-\lambda} \lambda^x}{x!}\]

<p>One important limit is used to prove the approximation:</p>

\[\lim_{n\rightarrow \infty} (1 + \frac{r}{n})^n = e^r \text{ or, } \\
    \lim_{h\rightarrow 0} (1 + rh)^{1/h} = e^r\]

<p>To better see the connection between these two distributions, consider the binomial probability of seeing $x$ successes in $n$ trials, with the aforementioned probability of success, $p$, as shown below.</p>

\[P(x) = \binom{n}{x} p^x (1-p)^{n - x}\]

<p>Let denote the expected value of the binomial distribution as $\lambda = np$, which means $p = \frac{\lambda}{n}$. Here $\lambda$ is also the arriving rate of success event of the approximated Poisson distribution.</p>

<p>The probability mass function can be written as follows:</p>

\[P(x) = \binom{n}{x} (\frac{\lambda}{n})^x (1- \frac{\lambda}{n})^{n - x}\]

<p>Using the standard formula for the combinations of $n$ things taken $x$ at a time and some simple properties of exponents, we can further expand things to</p>

\[P(x) = \frac{n(n-1)\cdots (n-x+1)}{x!} \cdot \frac{\lambda^x}{n^x} (1- \frac{\lambda}{n})^{n - x}\]

<p>Notice that there are exactly xùë• factors in the numerator of the first fraction. Let us swap denominators between the first and second fractions, splitting the$ $n^x$ all of the factors of the first fraction‚Äôs numerator.</p>

\[P(x) = \frac{n}{n} \cdot \frac{n-1}{n} \cdots \frac{n-x+1}{n} \cdot \frac{\lambda^x}{x!} (1- \frac{\lambda}{n})^{n} \cdot (1- \frac{\lambda}{n})^{-x}\]

<p>Finally, applying the limit to $P(x)$ and keeping $x, \lambda$ Fiexed:</p>

\[\begin{align}
    \lim_{n\rightarrow\infty} P(x) = \frac{\lambda^x}{x!} \lim_{n\rightarrow\infty} (\frac{n}{n} \cdot \frac{n-1}{n} \cdots \frac{n-x+1}{n}) \cdot (1- \frac{\lambda}{n})^{-x} \cdot \lim_{n\rightarrow\infty} (1- \frac{\lambda}{n})^{n} \\
    = \frac{e^{-\lambda} \lambda^x}{x!}
    \end{align}\]

<h2 id="exponential-distribution">Exponential Distribution</h2>

<p>For the continuous version of exponential distribution, the probability density function (PDF):</p>

\[f(x; \lambda) =
\begin{cases}
    \lambda e^{-\lambda x} &amp; \text{if } x \ge  0 \\
    0 &amp; \text{if } x &lt; 0 
  \end{cases}\]

<p>Here $\lambda$ &gt; 0 is the parameter of the distribution, often called the <em>rate parameter</em>. The distribution is supported on the interval [0, $\infty$). If a <a href="https://en.wikipedia.org/wiki/Random_variable" rel="external nofollow noopener" target="_blank">random variable</a> <em>X</em> has this distribution, we write $X \sim \rm{Exp}(\lambda)$.</p>

<p>The cumulative distribution function:</p>

\[F(x; \lambda) =
\begin{cases}
    1 -  e^{-\lambda x} &amp; \text{if } x \ge  0 \\
    0 &amp; \text{if } x &lt; 0 
  \end{cases}\]

<p>The CDF and PDF are not the actual probability. For continuous probability distribution, we only compute the probability of a given interval instead of one single point. Because the probability $P(a \le x \le b)$ is the integral of PDF values in the range of $[a, b]$ and the area under a single point is infinitesimally small. The CDF is the integral of PDF, we can simple compute the probability as:</p>

\[P(a \le x \le b) = F(b) - F(a)\]

<p>where $F(x)$ is the CDF of $x$.</p>

<p>Property:</p>

<ul>
  <li>
<strong>Mean</strong>: $\frac{1}{\lambda}$</li>
  <li>
<strong>Variance</strong>: $\frac{1}{\lambda^2}$‚Äã</li>
  <li>
<strong>Memoryless</strong>: The exponential distribution and the <a href="https://en.wikipedia.org/wiki/Geometric_distribution" rel="external nofollow noopener" target="_blank">geometric distribution</a> are <a href="https://en.wikipedia.org/wiki/Memorylessness" rel="external nofollow noopener" target="_blank">the only memoryless probability distributions</a>.</li>
</ul>

<hr>

<h2 id="poisson-process">Poisson Process</h2>

<p>Let $\lambda &gt; 0$ be fixed. The counting process ${N(t), t \in [0, \infty)}$ is called a Poisson Process with <strong>rate $\lambda$</strong> if all the following conditions hold:</p>

<ul>
  <li>$N(0) = 0$;</li>
  <li>$N(t)$‚Äã has <strong>independent increments</strong>;</li>
  <li>The number of arrivals in any interval of length $\tau &gt; 0$ has $\rm{Poisson}(\lambda \tau)$ distribution.</li>
</ul>

<p>Note that from the above definition, we conclude that in a Poisson process, the distribution of the number of arrivals in any interval <strong>depends only on</strong> the length of the interval, <strong>and not on</strong> the exact location of the interval on the real line. Therefore the <em>Poisson process has stationary increments</em>.</p>

<p>We denote $X_1$ as the interval of the first arrival, the $T_1$ is the time instant of first arrival.</p>

<p><img src="/assets/img/posts/2024.06.17/Poisson-interarrival.png" alt="Arrivals" style="zoom:50%;"></p>

<p>If $N(t)$ is a Poisson process with rate $\lambda$, then the. interarrival times $X_1, X_2, \dots$ are independent and</p>

\[X_i \sim \rm{Exponential(\lambda)}\]

<p>$X$ is a memoryless random variable, that is</p>

\[P(X &gt; x + a | X &gt; a) = P(X &gt; x)\]

<p>Thinking of the Poisson process, the memoryless property of the interarrival times <strong>is consistent with the independent increment property of the Poisson distribution</strong>. In some sense, both are implying that the number of arrivals in non-overlapping intervals are independent.</p>

<p>Since the arrival time can be derived by:</p>

<p>\(\begin{align}
T_1 =X_1 \\
T_2 =X_1 + X_2 \\
T_3 =X_1 + X_2 + X_3
\end{align}\)
More specifically, $T_n$ is the sum of $n$ independent $\rm{Exponential}(\lambda)$ random variables. Then $T_n \sim \rm{Gamma(n, \lambda)}$. Note that here $n \in \mathbb{N}$. The $\rm{Gamma(n, \lambda)}$‚Äã is also called <strong>Erlang</strong> distribution, i.e, we can write</p>

\[T_n \sim \rm{Erlang}(n, \lambda) =  \rm{Gamma(n, \lambda)}\]

<p>The PDF of $T_n$, for $n = 1, 2, 3, \dots$, is given by</p>

\[f_{T_n}(t) = \frac{\lambda^nt^{n-1} e^{-\lambda t}}{(n-1)!}\]

<p>and</p>

\[E(T_n) = \frac{n}{\lambda} \\
Var(T_n) = \frac{n}{\lambda^2}\]

<hr>

<h2 id="markov-chain">Markov Chain</h2>

<p>Given the past states $X_0, X_1, \dots, X_n-1$ and the current state $X_n$, a Markov chain is a stochastic process, where the conditional distribution of any future state $X_{n+1}$ is independent of the <strong>past states</strong> and depends solely on <strong>current state</strong>. The $P_{ij}$ represents the probability of the process transiting from state $i$ to $j$, and the one-step transition matrix is a 2D matrix containing all possible probability of states transition. The multiple step transition can be obtained by the <strong>Chapman-Kolmogorov Equations</strong>:</p>

\[P^{n+m}_{ij} = \sum_{k=0}^{\infty} P_{ik}^n P_{kj}^m\]

<p>Two states $i$ and $j$ that are accessible to each other are called to <strong>communicate</strong>. For any state, the process can never change since once entered is an <strong>absorbing state</strong>. Two states that communicate with each other are said to be in the same <strong>class</strong>, each state class is separate with another one. The Markov chain is <strong>irreducible</strong> if there is only one class (i.e., all states communicate with each other).</p>

<h3 id="limiting-probabilities">Limiting Probabilities</h3>

<p>For an irreducible ergodic Markov (one class) chain $\lim_{n \rightarrow \infty} P^n_{ij}$ exists and is independent of $i$. It shows the limiting probability that the process will be in state $j$ at time $n$, which <strong>equals to the long-run proportion of time that the process will be in state $j$</strong>.</p>

<h3 id="mean-time-spent-in-transient-states">Mean Time Spent in Transient States</h3>

<p>Considering a finite state Markov chain with states in range $T={1, 2, \dots, t}$, the transition probabilities matrix is a $t \times t$ matrix $P_T$. We denote the identity matrix as $I$. For transient state $i, j$, let $s_{ij}$ denotes the expected <strong>number of time periods</strong> that the Markov chain is in state $j$, given it starts from state $i$. The matrix format for transient state times is $S$, we have:</p>

\[S = (I - P_T)^{-1}\]

<p>For each element, we can also compute the transition probability $f_{ij}$ from state $i$ to state $j$:</p>

\[f_{ij} = \frac{s_{ij} - \delta_{ij}}{s_{jj}}\]

<p>Where $\delta_{ij}$ is the corresponding element in identity matrix.</p>

  </article><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;">
  <script>
    let giscusTheme = localStorage.getItem("theme");
    let giscusAttributes = {
        "src": "https://giscus.app/client.js",
        "data-repo": "kobeHub/kobehub.github.io",
        "data-repo-id": "MDEwOlJlcG9zaXRvcnkxNjQyMzA0MjU=",
        "data-category": "Comments",
        "data-category-id": "DIC_kwDOCcn1Gc4CUFvv",
        "data-mapping": "title",
        "data-strict": "1",
        "data-reactions-enabled": "1",
        "data-emit-metadata": "0",
        "data-input-position": "bottom",
        "data-theme": giscusTheme,
        "data-lang": "en",
        "crossorigin": "anonymous",
        "async": "",
    };


    let giscusScript = document.createElement("script");
    Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
    document.getElementById("giscus_thread").appendChild(giscusScript);
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a>
</noscript>
</div>
</div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        ¬© Copyright 2025 Inno (Dong)  Jia. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script defer src="/assets/js/common.js"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
